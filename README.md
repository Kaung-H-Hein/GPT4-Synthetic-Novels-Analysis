# GPT4-Synthetic-Novels-Analysis
This repository presents a comprehensive project that applies natural language processing (NLP) techniques to a dataset of 700 synthetic short stories generated by OpenAI’s GPT-4 model. Each story is annotated with metadata, which includes information that can be used to train and evaluate models. The project, part of my MSc in AI studies, is divided into three sections:

**Section 1: Text Processing and Linguistic Insights**
- 1.1 Zipf’s Law
- 1.2 Writing Style and Dependency Parsing

**Section 2: Text Classification**
- 2.1 Text Classification with Word2Vec
- 2.2 Text Classification with BERT

**Section 3: Sentiment and Emotion**
- 3.1 Sentiment Analysis with BERT
- 3.2 Emotional Content of Stories

---

## Section 1: Text Processing and Linguistic Insights
This section investigates key aspects of the linguistic properties of synthetic text generated by GPT-4 model, focusing on Zipf’s law and writing style. 

### Key Libraries and Tools Used:
- **spaCy** for tokenization and dependency parsing.
- **Matplotlib** for data visualisation.
- **ptitprince** for raincloud plots.
- **NumPy** and **collections.Counter** for data manipulation.

### 1.1 Zipf’s Law
Zipf’s law is an empirical observation stating that in a given corpus, the frequency of a word is inversely proportional to its rank in the frequency table. 
This law holds true for most natural languages, where the most common word occurs roughly twice as often as the second most common word, three times as often as the third most common, and so on.

**Aim**:  
The objective is to determine if Zipf’s law holds for synthetic text generated by GPT-4. The text is treated as a single corpus, and the analysis checks whether Zipf’s law holds by calculating token frequencies, ranking the tokens, and visualising the results.

**Overview**:  
The steps followed in this analysis include:
1. **Data exploration and preparation**:
   - Collecting story data and concatenating the text contents into a single corpus.
   - Applying the `en_core_web_sm` English tokenizer, tagger, and parser from `spaCy`.
   - Using regular expressions to remove punctuation.
   - Processing the text with the `spaCy` NLP pipeline, extracting tokens.
2. **Analysis**:
   - Calculating token frequencies and rankings.
   - Visualising the frequency distribution.
   - Evaluating whether Zipf’s law holds for the synthetic text.

### 1.2 Writing Style and Dependency Parsing
The text for each story was generated in the style of one of five authors: Samuel Beckett, Stephen King, Lewis Carroll, Charles Dickens, and Enid Blyton. 
The author used for each story is provided in the “writer” metadata field.

**Aim**:  
The goal is to explore whether there are differences in the linguistic structure of stories generated by different authors. Specifically, the syntactic complexity of sentences is investigated by examining the longest dependency span in each sentence. This measure (`max_dep`) serves as an indicator of syntactic complexity, and `max_dep` is compared across the five authors.

**Overview**:  
The steps followed in this analysis include:
1. **Data exploration**:
   - Retrieving the writer metadata.
   - Selecting stories based on the author metadata.
2. **Analysis**:
   - Calculating the longest dependency span for each sentence.
   - Calculating the average and maximum dependency spans for each story.
   - Visualising the results, using raincloud plots to compare the syntactic complexity across authors.

---

## Section 2: Text Classification  
This section focuses on developing a classifier to predict the `story_subject` metadata, which indicates the protagonist's profession in the story. The roles include "doctor," "local_politician," "rockstar," "scientist," and "taxi_driver."

### Key Libraries and Tools Used:
- **Data Processing**: `pandas`, `numpy`, `spacy`, `re`, `nltk`
- **Word Embeddings**: `gensim`, `KeyedVectors`
- **Pre-trained Models**: `transformers`, `BertTokenizer`, `BertModel`, `AutoTokenizer`, `AutoModelForSequenceClassification`
- **Machine Learning**: `sklearn`, `LogisticRegression`, `classification_report`, `confusion_matrix`
- **Deep Learning**: `torch`, `torch.nn`, `DataLoader`, `Dataset`

### 2.1 Text Classification with Word2Vec  

**Overview**: 
The following steps were undertaken to build and evaluate the classifier:  
- **Data Preprocessing**: Cleaning and preparing the text data for analysis.  
- **Label Encoding**: Converting categorical labels into numerical format.  
- **Tokenizing**: Using the `en_core_web_sm` English tokenizer from `spaCy`.  
- **Feature Extraction**: Generating feature vectors using the pre-trained `word2vec-google-news-300` model.  
- **Model Building**: Training a logistic regression model on the feature vectors.  
- **Model Evaluation**: Assessing the classifier's performance using a confusion matrix and a classification report.

**Outcome**: 
The Word2Vec-based model achieved a macro average accuracy of 0.69, showing promising performance and demonstrating the potential of word embeddings for text classification tasks.

### 2.2 Text Classification with BERT
This section leverages the pre-trained `bert-base-uncased` model for text classification. 

**Overview**: 
The process involved:
- **Dataset Preparation**: Text and labels were tokenised, padded, and structured into training, validation, and test datasets using the BERT tokenizer.
- **Model Architecture**: A custom BERT-based classifier was fine-tuned, featuring a dropout layer and a fully connected output layer for classification.
- **Training and Optimisation**: The model was trained for 20 epochs using the AdamW optimiser and a linear learning rate scheduler.
- **Evaluation**: Performance was assessed on validation and test sets using accuracy, precision, recall, and F1-score.

**Outcome**: 
The fine-tuning of BERT demonstrated its strong ability to understand context, leading to a significant improvement in text classification accuracy, achieving a macro average accuracy of 0.96.

---

## Section 3: Sentiment and Emotion
This section examines the sentiment and emotional content of the stories, focusing on predicting the endings and analyzing the emotional characteristics of each protagonist based on their associated metadata.

### Key Libraries and Tools Used:
- **Data Processing**: `pandas`, `numpy`, `spacy`, `re`, `nltk`
- **Pre-trained Models**: `transformers`, `BertTokenizer`, `BertModel`, `AutoTokenizer`, `AutoModelForSequenceClassification`
- **Machine Learning**: `sklearn`, `LabelEncoder`, `train_test_split`, `LogisticRegression`, `accuracy_score`, `classification_report`, `confusion_matrix`
- **Deep Learning**: `torch`, `torch.nn`, `DataLoader`, `Dataset`
- **Distance Metrics**: `scipy`, `euclidean`
- **Visualization**: `matplotlib`, `seaborn`
- **Collections**: `collections`, `defaultdict`

### 3.1 Sentiment Analysis with BERT
The stories are designed with three possible endings: happy, sad, and unexpected, which are recorded in the "ending" field of the metadata.

**Aim**: Fine-tune a pre-trained BERT model `bert-base-uncased` using only the last sentence of each story to predict the type of ending.

**Overview**:
- **Dataset Preparation**: Text data was cleaned, preprocessed, and split into training, validation, and test sets. The last sentence of each story was tokenized using the BERT tokenizer.
- **Label Encoding**: The categorical story endings were converted into numerical values suitable for model training.
- **Model Architecture**: A custom BERT-based classifier was fine-tuned with a dropout layer to prevent overfitting and a fully connected output layer for classification.
- **Training and Optimisation**: The model was trained for 10 epochs with the AdamW optimizer, utilizing a dynamic learning rate adjustment via a linear scheduler.
- **Loss Calculation**: Cross-entropy loss was employed to evaluate the model’s performance during training.
- **Evaluation**: The model's performance was assessed on the validation and test sets.

**Outcome**: 
Fine-tuning the BERT model achieved a macro average accuracy of 0.69, demonstrating the model’s potential for predicting story endings. However, there is room for improvement, and further fine-tuning and data refinement could enhance accuracy.

### 3.2 Emotional Content of Stories
A pre-trained emotion detection model, `bert-base-uncased-emotion`, from Hugging Face was employed to generate a probability distribution for each sentence in the stories. These probability vectors were then averaged to create an emotion vector representing the emotional content of each story. The "quality" metadata, associated with the protagonist, contains categories such as "angry," "depressed," "excited," "rich," "poor," "old," and "young."

**Aim**: To investigate how the emotional content of the stories varies across the different "quality" categories.

**Overview**:
- **Data Preparation**: Clean and preprocess the data, ensuring the stories and their respective metadata are properly formatted for analysis.
- **Emotion Detection**: Use the bert-base-uncased-emotion model to obtain a probability distribution for each sentence in every story.
- **Emotion Vector Creation**: Average the probability distributions to generate a single emotion vector for each story, representing its overall emotional content.
- **Metadata Pairing**: Pair the "quality" metadata of each story's protagonist with the corresponding emotion vector.
- **Analysis**: Investigate the differences in emotional content across the various "quality" categories using Euclidean distance and cosine similarity.
- **Cluster Analysis**: Perform clustering analysis to explore potential patterns in how emotional content aligns with the "quality" categories.

**Outcome**: 
This analysis aims to uncover whether and how the emotional content of stories varies based on the protagonist’s "quality" attributes, potentially revealing insights into the emotional profiles of different character types.
